{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "compare_models.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Jic-5GT_i2C"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate more-itertools unidiff optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "momkjuBvCQ-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.cuda\n",
        "\n",
        "for i in range(torch.cuda.device_count()):\n",
        "  print(\"device {0}: {1}\".format(i, torch.cuda.get_device_name(i)))\n",
        "  print(\" capability: {0}\".format(torch.cuda.get_device_capability(i)))\n",
        "  print(\" properties: {0}\".format(torch.cuda.get_device_properties(i)))"
      ],
      "metadata": {
        "id": "WkiIJJ52CVGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import time\n",
        "sys.path.append('/content/drive/MyDrive/laredo/llm-research/src')\n",
        "sys.path.append('/content/drive/MyDrive/laredo/llm-research/src/eval')\n",
        "import test_runner\n",
        "\n",
        "def summarize(df): \n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "\n",
        "  sns.set_style('dark')\n",
        "\n",
        "  percentiles=[0.01, 0.05, 0.1, .25, .75, .9, .95, .99]\n",
        "\n",
        "  display(\"Overall eval results\")\n",
        "  df = df.sort_values(by=['eval_idx'])\n",
        "\n",
        "  display(df.groupby(['name'])['score'].describe(percentiles=percentiles))\n",
        "\n",
        "  with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', 400): \n",
        "    display(\"Eval results grouped by test case location.\")\n",
        "    display(df.groupby(['name', 'location'])['score'].describe(percentiles=percentiles))\n",
        "\n",
        "  mean_global = df.groupby(['name'])['score'].mean().to_frame('global').transpose()\n",
        "  mean_by_loc = pd.pivot_table(df, values='score', index='location', columns=['name'])\n",
        "\n",
        "  table = pd.concat([mean_global, mean_by_loc])\n",
        "\n",
        "  display (table) \n",
        "\n",
        "  table.plot(kind='bar', legend=True)\n",
        "  plt.rcParams['figure.figsize'] = [15, 15]\n",
        "  plt.title(\"Mean by evalset and test location\")\n",
        "  plt.xticks(rotation=30, horizontalalignment='center')\n",
        "  plt.xlabel(\"location\")\n",
        "  plt.ylabel(\"mean tokens\")\n",
        "\n",
        "\n",
        "def diff_frames(a, b, df, rows=10):\n",
        "  sliced_frames = df[(df.name.str.startswith(a)) | (df.name.str.startswith(b))]\n",
        "  sliced_frames = sliced_frames.sort_values(by=['eval_idx', 'commit_hash', 'location', 'instance']).reset_index(drop=True)\n",
        "  sliced_frames['diff'] = sliced_frames.groupby(['commit_hash', 'location', 'instance'])['score'].diff()\n",
        "\n",
        "  diffs = sliced_frames[['commit_hash', 'location', 'instance', 'score', 'diff']].dropna().sort_values(by='diff', ascending=False)\n",
        "\n",
        "  with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', 100): \n",
        "    display(diffs.head(rows))\n",
        "    display(diffs.tail(rows))\n",
        "\n",
        "\n",
        "def eval_datasets(tuples, split=\"validation\"): \n",
        "  frames = []\n",
        "  for idx, t in enumerate(tuples): \n",
        "    dataset_path = f\"/content/drive/MyDrive/laredo/data/{t[2]}\"\n",
        "\n",
        "    print(f\"Starting evaluation of {t[1]} with {dataset_path}[{split}]\")\n",
        "\n",
        "    df = test_runner.eval_dataset(t[0], tokenizer, dataset_path, split)\n",
        "    df['name'] = f\"{t[1]} - {t[2]}\"\n",
        "    df['eval_idx'] = idx;\n",
        "    frames.append(df)\n",
        "\n",
        "  return pd.concat(frames)"
      ],
      "metadata": {
        "id": "A4G0tk49CpGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/laredo/models/PySnooper\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"/content/drive/MyDrive/laredo/tokenizer\")\n",
        "\n",
        "codeparrot_model = GPT2LMHeadModel.from_pretrained(\"lvwerra/codeparrot-small\")\n",
        "codeparrot_model = codeparrot_model.to(device)\n",
        "codeparrot_model.config.update({ \"reorder_and_upcast_attn\": True, \"scale_attn_by_inverse_layer_idx\": True})\n",
        "codeparrot_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "pysnooper_model = GPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/laredo/models/PySnooper\")\n",
        "pysnooper_model = pysnooper_model.to(device)\n",
        "\n",
        "background_model = GPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/laredo/models/background-20220330\")\n",
        "background_model = background_model.to(device)\n",
        "\n",
        "df = eval_datasets(\n",
        "    [(codeparrot_model, 'CodeParrot Model', 'PySnooper'), \n",
        "     (pysnooper_model, 'PySnooper Model', 'PySnooper')]\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JmwImH0ADd7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize(df)"
      ],
      "metadata": {
        "id": "-1CsEZOYysV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diff_frames(\"CodeParrot Model\", \"PySnooper Model\", df)  "
      ],
      "metadata": {
        "id": "36NHVneepkPH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}